{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lumbar Spine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1000)\n",
    "import ast\n",
    "import os\n",
    "import collections\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LightSource\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conformal_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../files/lumbar-test-pred.csv')\n",
    "df = df.replace(-1, np.NaN)\n",
    "# df = pd.read_csv('../files/lumbar-test-pred.csv')\n",
    "\n",
    "# exclude first verterbral level as it contains many missing scores\n",
    "exclude = [\n",
    "    'Unnamed: 0', \n",
    "]\n",
    "df.drop(columns=exclude, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T12-L1 Left: 66%\n",
      "L1-L2 Left: 88%\n",
      "L2-L3 Left: 80%\n",
      "L3-L4 Left: 69%\n",
      "L4-L5 Left: 61%\n",
      "L5-S1 Left: 66%\n",
      "\n",
      "T12-L1 right: 66%\n",
      "L1-L2 right: 89%\n",
      "L2-L3 right: 79%\n",
      "L3-L4 right: 69%\n",
      "L4-L5 right: 65%\n",
      "L5-S1 right: 66%\n",
      "\n",
      "T12-L1 center: 67%\n",
      "L1-L2 center: 89%\n",
      "L2-L3 center: 83%\n",
      "L3-L4 center: 75%\n",
      "L4-L5 center: 73%\n",
      "L5-S1 center: 85%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_pred = lambda x: np.argmax(ast.literal_eval(x)) if x == x else x\n",
    "print(f'T12-L1 Left: {(df.true_T12_L1_left == df.score_T12_L1_left.map(get_pred)).sum() / df.true_T12_L1_left.count():.0%}')\n",
    "print(f'L1-L2 Left: {(df.true_L1_L2_left == df.score_L1_L2_left.map(get_pred)).sum() / df.true_L1_L2_left.count():.0%}')\n",
    "print(f'L2-L3 Left: {(df.true_L2_L3_left == df.score_L2_L3_left.map(get_pred)).sum() / df.true_L2_L3_left.count():.0%}')\n",
    "print(f'L3-L4 Left: {(df.true_L3_L4_left == df.score_L3_L4_left.map(get_pred)).sum() / df.true_L3_L4_left.count():.0%}')\n",
    "print(f'L4-L5 Left: {(df.true_L4_L5_left == df.score_L4_L5_left.map(get_pred)).sum() / df.true_L4_L5_left.count():.0%}')\n",
    "print(f'L5-S1 Left: {(df.true_L5_S1_left == df.score_L5_S1_left.map(get_pred)).sum() / df.true_L5_S1_left.count():.0%}')\n",
    "print()\n",
    "print(f'T12-L1 right: {(df.true_T12_L1_right == df.score_T12_L1_right.map(get_pred)).sum() / df.true_T12_L1_right.count():.0%}')\n",
    "print(f'L1-L2 right: {(df.true_L1_L2_right == df.score_L1_L2_right.map(get_pred)).sum() / df.true_L1_L2_right.count():.0%}')\n",
    "print(f'L2-L3 right: {(df.true_L2_L3_right == df.score_L2_L3_right.map(get_pred)).sum() / df.true_L2_L3_right.count():.0%}')\n",
    "print(f'L3-L4 right: {(df.true_L3_L4_right == df.score_L3_L4_right.map(get_pred)).sum() / df.true_L3_L4_right.count():.0%}')\n",
    "print(f'L4-L5 right: {(df.true_L4_L5_right == df.score_L4_L5_right.map(get_pred)).sum() / df.true_L4_L5_right.count():.0%}')\n",
    "print(f'L5-S1 right: {(df.true_L5_S1_right == df.score_L5_S1_right.map(get_pred)).sum() / df.true_L5_S1_right.count():.0%}')\n",
    "print()\n",
    "print(f'T12-L1 center: {(df.true_T12_L1_center == df.score_T12_L1_center.map(get_pred)).sum() / df.true_T12_L1_center.count():.0%}')\n",
    "print(f'L1-L2 center: {(df.true_L1_L2_center == df.score_L1_L2_center.map(get_pred)).sum() / df.true_L1_L2_center.count():.0%}')\n",
    "print(f'L2-L3 center: {(df.true_L2_L3_center == df.score_L2_L3_center.map(get_pred)).sum() / df.true_L2_L3_center.count():.0%}')\n",
    "print(f'L3-L4 center: {(df.true_L3_L4_center == df.score_L3_L4_center.map(get_pred)).sum() / df.true_L3_L4_center.count():.0%}')\n",
    "print(f'L4-L5 center: {(df.true_L4_L5_center == df.score_L4_L5_center.map(get_pred)).sum() / df.true_L4_L5_center.count():.0%}')\n",
    "print(f'L5-S1 center: {(df.true_L5_S1_center == df.score_L5_S1_center.map(get_pred)).sum() / df.true_L5_S1_center.count():.0%}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left 0.7178136318735594\n",
      "right 0.7257369990062935\n",
      "center 0.785427807486631\n"
     ]
    }
   ],
   "source": [
    "print('left', ((df.true_T12_L1_left == df.score_T12_L1_left.map(get_pred)).sum() + (df.true_L1_L2_left == df.score_L1_L2_left.map(get_pred)).sum()  + (df.true_L2_L3_left == df.score_L2_L3_left.map(get_pred)).sum() + (df.true_L3_L4_left == df.score_L3_L4_left.map(get_pred)).sum()  + (df.true_L4_L5_left == df.score_L4_L5_left.map(get_pred)).sum() + (df.true_L5_S1_left == df.score_L5_S1_left.map(get_pred)).sum()) / (df.true_T12_L1_left.count() + df.true_L1_L2_left.count() + df.true_L2_L3_left.count() + df.true_L3_L4_left.count() + df.true_L4_L5_left.count() + df.true_L5_S1_left.count()))\n",
    "print('right', ((df.true_T12_L1_right == df.score_T12_L1_right.map(get_pred)).sum() + (df.true_L1_L2_right == df.score_L1_L2_right.map(get_pred)).sum()  + (df.true_L2_L3_right == df.score_L2_L3_right.map(get_pred)).sum() + (df.true_L3_L4_right == df.score_L3_L4_right.map(get_pred)).sum()  + (df.true_L4_L5_right == df.score_L4_L5_right.map(get_pred)).sum() + (df.true_L5_S1_right == df.score_L5_S1_right.map(get_pred)).sum()) / (df.true_T12_L1_right.count() + df.true_L1_L2_right.count() + df.true_L2_L3_right.count() + df.true_L3_L4_right.count() + df.true_L4_L5_right.count() + df.true_L5_S1_right.count()))\n",
    "print('center', ((df.true_T12_L1_center == df.score_T12_L1_center.map(get_pred)).sum() + (df.true_L1_L2_center == df.score_L1_L2_center.map(get_pred)).sum()  + (df.true_L2_L3_center == df.score_L2_L3_center.map(get_pred)).sum() + (df.true_L3_L4_center == df.score_L3_L4_center.map(get_pred)).sum()  + (df.true_L4_L5_center == df.score_L4_L5_center.map(get_pred)).sum() + (df.true_L5_S1_center == df.score_L5_S1_center.map(get_pred)).sum()) / (df.true_T12_L1_center.count() + df.true_L1_L2_center.count() + df.true_L2_L3_center.count() + df.true_L3_L4_center.count() + df.true_L4_L5_center.count() + df.true_L5_S1_center.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cols = [x for x in df.columns if 'score_' in x]\n",
    "score_df = df[score_cols].dropna()  # drop patients with missing scores\n",
    "\n",
    "label_cols = [x for x in df.columns if 'true_' in x]\n",
    "label_df = df[label_cols]\n",
    "label_df = label_df[label_df.index.isin(score_df.index)]\n",
    "label_df = label_df[~(label_df == -1).all(1)]  # drop patients with no gradings\n",
    "label_df.rename(\n",
    "    columns={x: x.replace('true_', '').upper() + '_TRUE' for x in label_df.columns}, \n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "score_df = score_df[score_df.index.isin(label_df.index)]\n",
    "\n",
    "score_array = np.array([\n",
    "    ast.literal_eval(disc) for patient in score_df.values for disc in patient\n",
    "])\n",
    "label_array = label_df.values.flatten().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_map = {\n",
    "    i: [i * label_df.shape[1] + j for j, w in enumerate(v) if w == w] \n",
    "    for i, v in enumerate(label_df.values)\n",
    "}\n",
    "\n",
    "for k, v in patient_map.items():  # sanity check\n",
    "    err_msg = f'{k}= {len(v)}= {(label_df.iloc[k] == label_df.iloc[k]).sum()}'\n",
    "    assert len(v) == (label_df.iloc[k] == label_df.iloc[k]).sum(), err_msg\n",
    "\n",
    "clean_index = sum(patient_map.values(), [])\n",
    "clean_score_array = score_array[clean_index]\n",
    "clean_label_array = label_array[clean_index]\n",
    "\n",
    "clean_patient_map = {k: len(v) for k, v in patient_map.items()}\n",
    "\n",
    "start_index = 0\n",
    "end_index = 0\n",
    "for k, v in clean_patient_map.items():\n",
    "    end_index += v\n",
    "    clean_patient_map[k] = list(range(start_index, end_index))\n",
    "    start_index += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8030526834071886"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(clean_score_array.argmax(1) == clean_label_array).sum() / clean_label_array.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 1: comparison of conformal methods (LAC, CDF, APS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(conformal_demo)\n",
    "\n",
    "# Experimental parameters\n",
    "num_trials = 100\n",
    "\n",
    "# Range of  miscoverage rates\n",
    "alphas = np.array([0.01, 0.05, 0.10, 0.15, 0.2])\n",
    "\n",
    "# Percentage of data to use for calibration\n",
    "cal_percent = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_conditional_eval(sets, labels, classes=['neg', 'mod', 'mil', 'sev']):\n",
    "    res = collections.defaultdict()\n",
    "    for i, v in enumerate(classes):\n",
    "        class_cond_sets = sets[labels == i]\n",
    "        n = class_cond_sets.shape[0]\n",
    "        res[v] = {\n",
    "            'count': n,\n",
    "            'coverage': (class_cond_sets[np.arange(n), i].sum() / n) if n > 0 else 0,\n",
    "            'size': class_cond_sets.sum(1).mean(),\n",
    "        }\n",
    "    return res\n",
    "\n",
    "def size_conditional_eval(sets, labels, sizes=[1, 2, 3, 4]):\n",
    "    res = collections.defaultdict()\n",
    "    for i, v in enumerate(sizes):\n",
    "        size_cond_sets = sets[sets.sum(1) == v]\n",
    "        n = size_cond_sets.shape[0]\n",
    "        res[v] = {\n",
    "            'count': n,\n",
    "            'coverage': (size_cond_sets[np.arange(n), i].sum() / n) if n > 0 else 0,\n",
    "        }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_lac_overall_coverage = collections.defaultdict(list)\n",
    "alpha_cdf_overall_coverage = collections.defaultdict(list)\n",
    "alpha_aps_overall_coverage = collections.defaultdict(list)\n",
    "alpha_lac_overall_size = collections.defaultdict(list)\n",
    "alpha_cdf_overall_size = collections.defaultdict(list)\n",
    "alpha_aps_overall_size = collections.defaultdict(list)\n",
    "\n",
    "alpha_lac_class_cond_count = collections.defaultdict(list)\n",
    "alpha_cdf_class_cond_count = collections.defaultdict(list)\n",
    "alpha_aps_class_cond_count = collections.defaultdict(list)\n",
    "alpha_lac_class_cond_coverage = collections.defaultdict(list)\n",
    "alpha_cdf_class_cond_coverage = collections.defaultdict(list)\n",
    "alpha_aps_class_cond_coverage = collections.defaultdict(list)\n",
    "alpha_lac_class_cond_size = collections.defaultdict(list)\n",
    "alpha_cdf_class_cond_size = collections.defaultdict(list)\n",
    "alpha_aps_class_cond_size = collections.defaultdict(list)\n",
    "alpha_lac_size_cond_count = collections.defaultdict(list)\n",
    "alpha_cdf_size_cond_count = collections.defaultdict(list)\n",
    "alpha_aps_size_cond_count = collections.defaultdict(list)\n",
    "alpha_lac_size_cond_coverage = collections.defaultdict(list)\n",
    "alpha_cdf_size_cond_coverage = collections.defaultdict(list)\n",
    "alpha_aps_size_cond_coverage = collections.defaultdict(list)\n",
    "\n",
    "for alpha in alphas:\n",
    "    lac_overall_coverage = []\n",
    "    cdf_overall_coverage = []\n",
    "    aps_overall_coverage = []\n",
    "    lac_overall_size = []\n",
    "    cdf_overall_size = []\n",
    "    aps_overall_size = []\n",
    "\n",
    "    lac_class_cond_count = collections.defaultdict(list)\n",
    "    cdf_class_cond_count = collections.defaultdict(list)\n",
    "    aps_class_cond_count = collections.defaultdict(list)\n",
    "    lac_class_cond_coverage = collections.defaultdict(list)\n",
    "    cdf_class_cond_coverage = collections.defaultdict(list)\n",
    "    aps_class_cond_coverage = collections.defaultdict(list)\n",
    "    lac_class_cond_size = collections.defaultdict(list)\n",
    "    cdf_class_cond_size = collections.defaultdict(list)\n",
    "    aps_class_cond_size = collections.defaultdict(list)\n",
    "    lac_size_cond_count = collections.defaultdict(list)\n",
    "    cdf_size_cond_count = collections.defaultdict(list)\n",
    "    aps_size_cond_count = collections.defaultdict(list)\n",
    "    lac_size_cond_coverage = collections.defaultdict(list)\n",
    "    cdf_size_cond_coverage = collections.defaultdict(list)\n",
    "    aps_size_cond_coverage = collections.defaultdict(list)\n",
    "    lac_size_cond_size = collections.defaultdict(list)\n",
    "    cdf_size_cond_size = collections.defaultdict(list)\n",
    "    aps_size_cond_size = collections.defaultdict(list)\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        rand_patients = np.random.permutation(list(clean_patient_map.keys()))\n",
    "        n = len(rand_patients)\n",
    "        split = int(n * cal_percent)\n",
    "        cal_patients = rand_patients[:split]\n",
    "        val_patients = rand_patients[split:]\n",
    "        cal_indexes = sum([clean_patient_map[k] for k in cal_patients], [])\n",
    "        val_indexes = sum([clean_patient_map[k] for k in val_patients], [])\n",
    "        cal_scores = clean_score_array[cal_indexes]\n",
    "        cal_labels = clean_label_array[cal_indexes]\n",
    "        val_scores = clean_score_array[val_indexes]\n",
    "        val_labels = clean_label_array[val_indexes]\n",
    "        \n",
    "        # Naive LAC\n",
    "        lac_qhat = np.quantile(\n",
    "            conformal_demo.lac_score_function(\n",
    "                np.copy(cal_scores),\n",
    "                np.copy(cal_labels)\n",
    "            ),\n",
    "            np.ceil((cal_scores.shape[0] + 1)*(1 - alpha))/cal_scores.shape[0],\n",
    "            interpolation='higher',\n",
    "        )\n",
    "        lac_pred_sets = conformal_demo.lac_prediction(np.copy(val_scores), lac_qhat)\n",
    "        lac_overall_coverage.append(lac_pred_sets[np.arange(val_scores.shape[0]), val_labels].sum() / val_labels.shape[0])\n",
    "        lac_overall_size.append(lac_pred_sets.sum(1).mean())\n",
    "        for _class, cond_res in class_conditional_eval(lac_pred_sets, val_labels).items():\n",
    "            lac_class_cond_count[_class].append(cond_res['count'])\n",
    "            lac_class_cond_coverage[_class].append(cond_res['coverage'])\n",
    "            lac_class_cond_size[_class].append(cond_res['size'])\n",
    "        for _size, cond_res in size_conditional_eval(lac_pred_sets, val_labels).items():\n",
    "            lac_size_cond_count[_size].append(cond_res['count'])\n",
    "            lac_size_cond_coverage[_size].append(cond_res['coverage'])\n",
    "        \n",
    "        # Ordinal CDF\n",
    "        cdf_qhat = np.quantile(\n",
    "            conformal_demo.cdf_naive_ordinal_score_function(\n",
    "                np.copy(cal_scores),\n",
    "                np.copy(cal_labels)\n",
    "            ),\n",
    "            np.ceil((cal_scores.shape[0] + 1)*(1 - alpha))/cal_scores.shape[0],\n",
    "            interpolation='higher',\n",
    "        )\n",
    "        cdf_pred_sets = conformal_demo.cdf_naive_ordinal_prediction(np.copy(val_scores), cdf_qhat)\n",
    "        cdf_overall_coverage.append(cdf_pred_sets[np.arange(val_scores.shape[0]), val_labels].sum() / val_labels.shape[0])\n",
    "        cdf_overall_size.append(cdf_pred_sets.sum(1).mean())\n",
    "        \n",
    "        for _class, cond_res in class_conditional_eval(cdf_pred_sets, val_labels).items():\n",
    "            cdf_class_cond_count[_class].append(cond_res['count'])\n",
    "            cdf_class_cond_coverage[_class].append(cond_res['coverage'])\n",
    "            cdf_class_cond_size[_class].append(cond_res['size'])\n",
    "        for _size, cond_res in size_conditional_eval(cdf_pred_sets, val_labels).items():\n",
    "            cdf_size_cond_count[_size].append(cond_res['count'])\n",
    "            cdf_size_cond_coverage[_size].append(cond_res['coverage'])\n",
    "        \n",
    "        # Ordinal APS\n",
    "        aps_qhat = conformal_demo.get_qhat_ordinal_aps(\n",
    "            conformal_demo.ordinal_aps_prediction,\n",
    "            np.copy(cal_scores),\n",
    "            np.copy(cal_labels),\n",
    "            alpha\n",
    "        )\n",
    "        aps_pred_sets = conformal_demo.ordinal_aps_prediction(np.copy(val_scores), aps_qhat)\n",
    "        aps_overall_coverage.append(aps_pred_sets[np.arange(val_scores.shape[0]), val_labels].sum() / val_labels.shape[0])\n",
    "        aps_overall_size.append(aps_pred_sets.sum(1).mean())\n",
    "        for _class, cond_res in class_conditional_eval(aps_pred_sets, val_labels).items():\n",
    "            aps_class_cond_count[_class].append(cond_res['count'])\n",
    "            aps_class_cond_coverage[_class].append(cond_res['coverage'])\n",
    "            aps_class_cond_size[_class].append(cond_res['size'])\n",
    "        for _size, cond_res in size_conditional_eval(aps_pred_sets, val_labels).items():\n",
    "            aps_size_cond_count[_size].append(cond_res['count'])\n",
    "            aps_size_cond_coverage[_size].append(cond_res['coverage'])\n",
    "        \n",
    "    alpha_lac_overall_coverage[alpha] = lac_overall_coverage\n",
    "    alpha_cdf_overall_coverage[alpha] = cdf_overall_coverage\n",
    "    alpha_aps_overall_coverage[alpha] = aps_overall_coverage\n",
    "    alpha_lac_overall_size[alpha] = lac_overall_size\n",
    "    alpha_cdf_overall_size[alpha] = cdf_overall_size\n",
    "    alpha_aps_overall_size[alpha] = aps_overall_size\n",
    "        \n",
    "    alpha_lac_class_cond_count[alpha] = lac_class_cond_count\n",
    "    alpha_cdf_class_cond_count[alpha] = cdf_class_cond_count\n",
    "    alpha_aps_class_cond_count[alpha] = aps_class_cond_count\n",
    "    alpha_lac_class_cond_coverage[alpha] = lac_class_cond_coverage\n",
    "    alpha_cdf_class_cond_coverage[alpha] = cdf_class_cond_coverage\n",
    "    alpha_aps_class_cond_coverage[alpha] = aps_class_cond_coverage\n",
    "    alpha_lac_class_cond_size[alpha] = lac_class_cond_size\n",
    "    alpha_cdf_class_cond_size[alpha] = cdf_class_cond_size\n",
    "    alpha_aps_class_cond_size[alpha] = aps_class_cond_size\n",
    "    alpha_lac_size_cond_count[alpha] = lac_size_cond_count\n",
    "    alpha_cdf_size_cond_count[alpha] = cdf_size_cond_count\n",
    "    alpha_aps_size_cond_count[alpha] = aps_size_cond_count\n",
    "    alpha_lac_size_cond_coverage[alpha] = lac_size_cond_coverage\n",
    "    alpha_cdf_size_cond_coverage[alpha] = cdf_size_cond_coverage\n",
    "    alpha_aps_size_cond_coverage[alpha] = aps_size_cond_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lac_agg_coverage = np.array(list(alpha_lac_overall_coverage.values()))\n",
    "cdf_agg_coverage = np.array(list(alpha_cdf_overall_coverage.values()))\n",
    "aps_agg_coverage = np.array(list(alpha_aps_overall_coverage.values()))\n",
    "lac_agg_size = np.array(list(alpha_lac_overall_size.values()))\n",
    "cdf_agg_size = np.array(list(alpha_cdf_overall_size.values()))\n",
    "aps_agg_size = np.array(list(alpha_aps_overall_size.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 26\n",
    "lw=4\n",
    "lw_aps=lw + 2\n",
    "elinewidth=3\n",
    "capsize=6\n",
    "alpha=0.7\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 3), sharex=True, sharey=False)\n",
    "\n",
    "plt.setp(\n",
    "    ax, \n",
    "    xlim=(0.0, 0.21),\n",
    ")\n",
    "\n",
    "ax[0].set_xlabel('Alpha', fontsize=fontsize)\n",
    "ax[0].set_ylabel('Coverage', fontsize=fontsize)\n",
    "# ax[0].set_title('Naive LAC', fontsize=fontsize + 6)\n",
    "ax[0].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "# ax[0].set_ylim(0.79, 1.0)\n",
    "# ax[0].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0].grid(ls='--')\n",
    "ax[0].errorbar(\n",
    "    alphas, aps_agg_coverage.mean(1), yerr=aps_agg_coverage.std(1), ls='-', lw=lw_aps, elinewidth=elinewidth,capsize=capsize,alpha=alpha,\n",
    ")\n",
    "ax[0].errorbar(\n",
    "    alphas, lac_agg_coverage.mean(1), yerr=lac_agg_coverage.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[0].errorbar(\n",
    "    alphas, cdf_agg_coverage.mean(1), yerr=cdf_agg_coverage.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[1].set_xlabel('Alpha', fontsize=fontsize)\n",
    "ax[1].set_ylabel('Set size', fontsize=fontsize)\n",
    "ax[1].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[1].set_ylim(0.9, 4.)\n",
    "ax[1].set_yticks(list(range(1, 5)), fontsize=fontsize)\n",
    "# ax[1].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[1].grid(ls='--')\n",
    "ax[1].errorbar(\n",
    "    alphas, aps_agg_size.mean(1), yerr=aps_agg_size.std(1), ls='-', lw=lw_aps, elinewidth=elinewidth,capsize=capsize,alpha=alpha,\n",
    "    label='Ordinal APS',\n",
    ")\n",
    "ax[1].errorbar(\n",
    "    alphas, lac_agg_size.mean(1), yerr=lac_agg_size.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    "    label='Naive LAC',\n",
    ")\n",
    "ax[1].errorbar(\n",
    "    alphas, cdf_agg_size.mean(1), yerr=cdf_agg_size.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    "    label='Ordinal CDF',\n",
    ")\n",
    "\n",
    "fig.legend(fontsize=fontsize-4, bbox_to_anchor=(1.02, 0.02), ncol=3)\n",
    "fig.tight_layout()\n",
    "os.makedirs('../figures/',exist_ok=True)\n",
    "plt.savefig('../figures/line-aggregate-comparison.eps', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lac_sev_coverage = np.array([alpha_lac_class_cond_coverage[alpha]['sev'] for alpha in alphas])\n",
    "lac_mod_coverage = np.array([alpha_lac_class_cond_coverage[alpha]['mod'] for alpha in alphas])\n",
    "lac_mil_coverage = np.array([alpha_lac_class_cond_coverage[alpha]['mil'] for alpha in alphas])\n",
    "lac_neg_coverage = np.array([alpha_lac_class_cond_coverage[alpha]['neg'] for alpha in alphas])\n",
    "cdf_sev_coverage = np.array([alpha_cdf_class_cond_coverage[alpha]['sev'] for alpha in alphas])\n",
    "cdf_mod_coverage = np.array([alpha_cdf_class_cond_coverage[alpha]['mod'] for alpha in alphas])\n",
    "cdf_mil_coverage = np.array([alpha_cdf_class_cond_coverage[alpha]['mil'] for alpha in alphas])\n",
    "cdf_neg_coverage = np.array([alpha_cdf_class_cond_coverage[alpha]['neg'] for alpha in alphas])\n",
    "aps_sev_coverage = np.array([alpha_aps_class_cond_coverage[alpha]['sev'] for alpha in alphas])\n",
    "aps_mod_coverage = np.array([alpha_aps_class_cond_coverage[alpha]['mod'] for alpha in alphas])\n",
    "aps_mil_coverage = np.array([alpha_aps_class_cond_coverage[alpha]['mil'] for alpha in alphas])\n",
    "aps_neg_coverage = np.array([alpha_aps_class_cond_coverage[alpha]['neg'] for alpha in alphas])\n",
    "\n",
    "lac_sev_size = np.array([alpha_lac_class_cond_size[alpha]['sev'] for alpha in alphas])\n",
    "lac_mod_size = np.array([alpha_lac_class_cond_size[alpha]['mod'] for alpha in alphas])\n",
    "lac_mil_size = np.array([alpha_lac_class_cond_size[alpha]['mil'] for alpha in alphas])\n",
    "lac_neg_size = np.array([alpha_lac_class_cond_size[alpha]['neg'] for alpha in alphas])\n",
    "cdf_sev_size = np.array([alpha_cdf_class_cond_size[alpha]['sev'] for alpha in alphas])\n",
    "cdf_mod_size = np.array([alpha_cdf_class_cond_size[alpha]['mod'] for alpha in alphas])\n",
    "cdf_mil_size = np.array([alpha_cdf_class_cond_size[alpha]['mil'] for alpha in alphas])\n",
    "cdf_neg_size = np.array([alpha_cdf_class_cond_size[alpha]['neg'] for alpha in alphas])\n",
    "aps_sev_size = np.array([alpha_aps_class_cond_size[alpha]['sev'] for alpha in alphas])\n",
    "aps_mod_size = np.array([alpha_aps_class_cond_size[alpha]['mod'] for alpha in alphas])\n",
    "aps_mil_size = np.array([alpha_aps_class_cond_size[alpha]['mil'] for alpha in alphas])\n",
    "aps_neg_size = np.array([alpha_aps_class_cond_size[alpha]['neg'] for alpha in alphas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 28\n",
    "lw=6\n",
    "elinewidth=3\n",
    "capsize=6\n",
    "alpha=0.7\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(20, 6), sharex=True, sharey=False)\n",
    "\n",
    "plt.setp(\n",
    "    ax, \n",
    "    xlim=(0.0, 0.21),\n",
    ")\n",
    "\n",
    "ax[0, 0].set_ylabel('Coverage', fontsize=fontsize)\n",
    "ax[0, 0].set_title('No stenosis', fontsize=fontsize )\n",
    "ax[0, 0].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[0, 0].set_ylim(0.4, 1.02)\n",
    "# ax[0, 0].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0, 0].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0, 0].grid(ls='--')\n",
    "ax[0, 0].errorbar(\n",
    "    alphas, aps_neg_coverage.mean(1), yerr=aps_neg_coverage.std(1), ls='-', lw=lw, elinewidth=elinewidth,capsize=capsize, alpha=alpha,\n",
    ")\n",
    "ax[0, 0].errorbar(\n",
    "    alphas, lac_neg_coverage.mean(1), yerr=lac_neg_coverage.std(1), ls=':', lw=lw, elinewidth=elinewidth, capsize=capsize,\n",
    ")\n",
    "ax[0, 0].errorbar(\n",
    "    alphas, cdf_neg_coverage.mean(1), yerr=cdf_neg_coverage.std(1), ls='--', lw=lw, elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[0, 1].set_title('Mild stenosis', fontsize=fontsize)\n",
    "ax[0, 1].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[0, 1].set_ylim(0.4, 1.02)\n",
    "ax[0, 1].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0, 1].grid(ls='--')\n",
    "ax[0, 1].errorbar(\n",
    "    alphas, aps_mil_coverage.mean(1), yerr=aps_mil_coverage.std(1), ls='-', lw=lw, elinewidth=elinewidth,capsize=capsize,alpha=alpha,\n",
    ")\n",
    "ax[0, 1].errorbar(\n",
    "    alphas, lac_mil_coverage.mean(1), yerr=lac_mil_coverage.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[0, 1].errorbar(\n",
    "    alphas, cdf_mil_coverage.mean(1), yerr=cdf_mil_coverage.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[0, 2].set_title('Moderate stenosis', fontsize=fontsize )\n",
    "ax[0, 2].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[0, 2].set_ylim(0.4, 1.02)\n",
    "ax[0, 2].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0, 2].grid(ls='--')\n",
    "ax[0, 2].errorbar(\n",
    "    alphas, aps_mod_coverage.mean(1), yerr=aps_mod_coverage.std(1), ls='-', lw=lw, elinewidth=elinewidth,capsize=capsize,alpha=alpha,\n",
    ")\n",
    "ax[0, 2].errorbar(\n",
    "    alphas, lac_mod_coverage.mean(1), yerr=lac_mod_coverage.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[0, 2].errorbar(\n",
    "    alphas, cdf_mod_coverage.mean(1), yerr=cdf_mod_coverage.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[0, 3].set_title('Severe stenosis', fontsize=fontsize )\n",
    "ax[0, 3].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[0, 3].set_ylim(0.4, 1.02)\n",
    "ax[0, 3].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0, 3].grid(ls='--')\n",
    "ax[0, 3].errorbar(\n",
    "    alphas, aps_sev_coverage.mean(1), yerr=aps_sev_coverage.std(1), ls='-', lw=lw, elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[0, 3].errorbar(\n",
    "    alphas, lac_sev_coverage.mean(1), yerr=lac_sev_coverage.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[0, 3].errorbar(\n",
    "    alphas, cdf_sev_coverage.mean(1), yerr=cdf_sev_coverage.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[1, 0].set_ylabel('Set size', fontsize=fontsize)\n",
    "ax[1, 0].set_xlabel('Alpha', fontsize=fontsize)\n",
    "ax[1, 0].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[1, 0].set_ylim(0.9, 4.1)\n",
    "ax[1, 0].set_yticks(list(range(1, 5)), fontsize=fontsize)\n",
    "# ax[1, 0].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[1, 0].grid(ls='--')\n",
    "ax[1, 0].errorbar(\n",
    "    alphas, aps_neg_size.mean(1), yerr=aps_neg_size.std(1), ls='-', lw=lw, elinewidth=elinewidth,capsize=capsize, alpha=alpha,\n",
    ")\n",
    "ax[1, 0].errorbar(\n",
    "    alphas, lac_neg_size.mean(1), yerr=lac_neg_size.std(1), ls=':', lw=lw, elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[1, 0].errorbar(\n",
    "    alphas, cdf_neg_size.mean(1), yerr=cdf_neg_size.std(1), ls='--', lw=lw, elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[1, 1].set_xlabel('Alpha', fontsize=fontsize)\n",
    "ax[1, 1].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[1, 1].set_ylim(0.9, 4.1)\n",
    "ax[1, 1].set_yticks(list(range(1, 5)), fontsize=fontsize)\n",
    "# ax[1, 1].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[1, 1].grid(ls='--')\n",
    "ax[1, 1].errorbar(\n",
    "    alphas, aps_mil_size.mean(1), yerr=aps_mil_size.std(1), ls='-', lw=lw, elinewidth=elinewidth,capsize=capsize,alpha=alpha,\n",
    ")\n",
    "ax[1, 1].errorbar(\n",
    "    alphas, lac_mil_size.mean(1), yerr=lac_mil_size.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[1, 1].errorbar(\n",
    "    alphas, cdf_mil_size.mean(1), yerr=cdf_mil_size.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[1, 2].set_xlabel('Alpha', fontsize=fontsize)\n",
    "ax[1, 2].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[1, 2].set_ylim(0.9, 4.1)\n",
    "ax[1, 2].set_yticks(list(range(1, 5)), fontsize=fontsize)\n",
    "# ax[1, 2].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[1, 2].grid(ls='--')\n",
    "ax[1, 2].errorbar(\n",
    "    alphas, aps_mod_size.mean(1), yerr=aps_mod_size.std(1), ls='-', lw=lw, elinewidth=elinewidth,capsize=capsize,alpha=alpha,\n",
    ")\n",
    "ax[1, 2].errorbar(\n",
    "    alphas, lac_mod_size.mean(1), yerr=lac_mod_size.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[1, 2].errorbar(\n",
    "    alphas, cdf_mod_size.mean(1), yerr=cdf_mod_size.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[1, 3].set_xlabel('Alpha', fontsize=fontsize)\n",
    "ax[1, 3].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[1, 3].set_ylim(0.9, 4.1)\n",
    "ax[1, 3].set_yticks(list(range(1, 5)), fontsize=fontsize)\n",
    "# ax[1, 3].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[1, 3].grid(ls='--')\n",
    "ax[1, 3].errorbar(\n",
    "    alphas, aps_sev_size.mean(1), yerr=aps_sev_size.std(1), ls='-', lw=lw,elinewidth=elinewidth,capsize=capsize, alpha=alpha,\n",
    "    label='Ordinal APS',\n",
    ")\n",
    "ax[1, 3].errorbar(\n",
    "    alphas, lac_sev_size.mean(1), yerr=lac_sev_size.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    "    label='Naive LAC',\n",
    ")\n",
    "ax[1, 3].errorbar(\n",
    "    alphas, cdf_sev_size.mean(1), yerr=cdf_sev_size.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    "    label='Ordinal CDF',\n",
    ")\n",
    "\n",
    "fig.legend(fontsize=fontsize + 8, bbox_to_anchor=(0.85, 0.02), ncol=3)\n",
    "fig.tight_layout()\n",
    "os.makedirs('../figures/',exist_ok=True)\n",
    "plt.savefig('../figures/line-comparison-by-class.eps', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lac_1_coverage = np.array([alpha_lac_size_cond_coverage[alpha][1] for alpha in alphas])\n",
    "lac_2_coverage = np.array([alpha_lac_size_cond_coverage[alpha][2] for alpha in alphas])\n",
    "lac_3_coverage = np.array([alpha_lac_size_cond_coverage[alpha][3] for alpha in alphas])\n",
    "lac_4_coverage = np.array([alpha_lac_size_cond_coverage[alpha][4] for alpha in alphas])\n",
    "cdf_1_coverage = np.array([alpha_cdf_size_cond_coverage[alpha][1] for alpha in alphas])\n",
    "cdf_2_coverage = np.array([alpha_cdf_size_cond_coverage[alpha][2] for alpha in alphas])\n",
    "cdf_3_coverage = np.array([alpha_cdf_size_cond_coverage[alpha][3] for alpha in alphas])\n",
    "cdf_4_coverage = np.array([alpha_cdf_size_cond_coverage[alpha][4] for alpha in alphas])\n",
    "aps_1_coverage = np.array([alpha_aps_size_cond_coverage[alpha][1] for alpha in alphas])\n",
    "aps_2_coverage = np.array([alpha_aps_size_cond_coverage[alpha][2] for alpha in alphas])\n",
    "aps_3_coverage = np.array([alpha_aps_size_cond_coverage[alpha][3] for alpha in alphas])\n",
    "aps_4_coverage = np.array([alpha_aps_size_cond_coverage[alpha][4] for alpha in alphas])\n",
    "\n",
    "lac_1_count = np.array([alpha_lac_size_cond_count[alpha][1] for alpha in alphas])\n",
    "lac_2_count = np.array([alpha_lac_size_cond_count[alpha][2] for alpha in alphas])\n",
    "lac_3_count = np.array([alpha_lac_size_cond_count[alpha][3] for alpha in alphas])\n",
    "lac_4_count = np.array([alpha_lac_size_cond_count[alpha][4] for alpha in alphas])\n",
    "cdf_1_count = np.array([alpha_cdf_size_cond_count[alpha][1] for alpha in alphas])\n",
    "cdf_2_count = np.array([alpha_cdf_size_cond_count[alpha][2] for alpha in alphas])\n",
    "cdf_3_count = np.array([alpha_cdf_size_cond_count[alpha][3] for alpha in alphas])\n",
    "cdf_4_count = np.array([alpha_cdf_size_cond_count[alpha][4] for alpha in alphas])\n",
    "aps_1_count = np.array([alpha_aps_size_cond_count[alpha][1] for alpha in alphas])\n",
    "aps_2_count = np.array([alpha_aps_size_cond_count[alpha][2] for alpha in alphas])\n",
    "aps_3_count = np.array([alpha_aps_size_cond_count[alpha][3] for alpha in alphas])\n",
    "aps_4_count = np.array([alpha_aps_size_cond_count[alpha][4] for alpha in alphas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(20, 6), sharex=True, sharey=False)\n",
    "\n",
    "plt.setp(\n",
    "    ax, \n",
    "    xlim=(0.0, 0.21),\n",
    ")\n",
    "\n",
    "ax[0, 0].set_ylabel('Coverage', fontsize=fontsize)\n",
    "ax[0, 0].set_title('Set size=1', fontsize=fontsize + 6)\n",
    "ax[0, 0].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[0, 0].set_ylim(0.0, 1.02)\n",
    "# ax[0, 0].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0, 0].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0, 0].grid(ls='--')\n",
    "ax[0, 0].errorbar(\n",
    "    alphas, aps_1_coverage.mean(1), yerr=aps_1_coverage.std(1), ls='-', lw=lw_aps,elinewidth=elinewidth,capsize=capsize, alpha=alpha,\n",
    ")\n",
    "ax[0, 0].errorbar(\n",
    "    alphas, lac_1_coverage.mean(1), yerr=lac_1_coverage.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[0, 0].errorbar(\n",
    "    alphas, cdf_1_coverage.mean(1), yerr=cdf_1_coverage.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[0, 1].set_title('Set size=2', fontsize=fontsize + 6)\n",
    "ax[0, 1].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[0, 1].set_ylim(0.0, 1.02)\n",
    "ax[0, 1].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0, 1].grid(ls='--')\n",
    "ax[0, 1].errorbar(\n",
    "    alphas, aps_2_coverage.mean(1), yerr=aps_2_coverage.std(1), ls='-', lw=lw_aps,elinewidth=elinewidth,capsize=capsize, alpha=alpha,\n",
    ")\n",
    "ax[0, 1].errorbar(\n",
    "    alphas, lac_2_coverage.mean(1), yerr=lac_2_coverage.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[0, 1].errorbar(\n",
    "    alphas, cdf_2_coverage.mean(1), yerr=cdf_2_coverage.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[0, 2].set_title('Set size=3', fontsize=fontsize + 6)\n",
    "ax[0, 2].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[0, 2].set_ylim(0.0, 1.02)\n",
    "ax[0, 2].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0, 2].grid(ls='--')\n",
    "ax[0, 2].errorbar(\n",
    "    alphas, aps_3_coverage.mean(1), yerr=aps_3_coverage.std(1), ls='-', lw=lw_aps, elinewidth=elinewidth,capsize=capsize,alpha=alpha,\n",
    ")\n",
    "ax[0, 2].errorbar(\n",
    "    alphas, lac_3_coverage.mean(1), yerr=lac_3_coverage.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[0, 2].errorbar(\n",
    "    alphas, cdf_3_coverage.mean(1), yerr=cdf_3_coverage.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[0, 3].set_title('Set size=4', fontsize=fontsize + 6)\n",
    "ax[0, 3].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "ax[0, 3].set_ylim(0.0, 1.02)\n",
    "ax[0, 3].yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[0, 3].grid(ls='--')\n",
    "ax[0, 3].errorbar(\n",
    "    alphas, aps_4_coverage.mean(1), yerr=aps_4_coverage.std(1), ls='-', lw=lw_aps,elinewidth=elinewidth,capsize=capsize, alpha=alpha,\n",
    ")\n",
    "ax[0, 3].errorbar(\n",
    "    alphas, lac_4_coverage.mean(1), yerr=lac_4_coverage.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[0, 3].errorbar(\n",
    "    alphas, cdf_4_coverage.mean(1), yerr=cdf_4_coverage.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[1, 0].set_ylabel('Count', fontsize=fontsize)\n",
    "ax[1, 0].set_xlabel('Alpha', fontsize=fontsize)\n",
    "# ax[1, 0].set_ylim(0, 10000)\n",
    "ax[1, 0].set_yscale('log')\n",
    "ax[1, 0].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "# ax[1, 0].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[1, 0].grid(ls='--')\n",
    "ax[1, 0].errorbar(\n",
    "    alphas, aps_1_count.mean(1), yerr=aps_1_count.std(1), ls='-', lw=lw_aps,elinewidth=elinewidth,capsize=capsize, alpha=alpha,\n",
    ")\n",
    "ax[1, 0].errorbar(\n",
    "    alphas, lac_1_count.mean(1), yerr=lac_1_count.std(1), ls=':', lw=lw, elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[1, 0].errorbar(\n",
    "    alphas, cdf_1_count.mean(1), yerr=cdf_1_count.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[1, 1].set_xlabel('Alpha', fontsize=fontsize)\n",
    "# ax[1, 1].set_ylim(0, 10000)\n",
    "ax[1, 1].set_yscale('log')\n",
    "ax[1, 1].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "# ax[1, 1].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[1, 1].grid(ls='--')\n",
    "ax[1, 1].errorbar(\n",
    "    alphas, aps_2_count.mean(1), yerr=aps_2_count.std(1), ls='-', lw=lw_aps,elinewidth=elinewidth,capsize=capsize, alpha=alpha,\n",
    ")\n",
    "ax[1, 1].errorbar(\n",
    "    alphas, lac_2_count.mean(1), yerr=lac_2_count.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[1, 1].errorbar(\n",
    "    alphas, cdf_2_count.mean(1), yerr=cdf_2_count.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "ax[1, 2].set_xlabel('Alpha', fontsize=fontsize)\n",
    "ax[1, 2].set_yscale('log')\n",
    "ax[1, 2].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "# ax[1, 2].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[1, 2].grid(ls='--')\n",
    "ax[1, 2].errorbar(\n",
    "    alphas, aps_3_count.mean(1), yerr=aps_3_count.std(1), ls='-', lw=lw_aps, elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[1, 2].errorbar(\n",
    "    alphas, lac_3_count.mean(1), yerr=lac_3_count.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "ax[1, 2].errorbar(\n",
    "    alphas, cdf_3_count.mean(1), yerr=cdf_3_count.std(1), ls='--', lw=lw, elinewidth=elinewidth,capsize=capsize,\n",
    ")\n",
    "\n",
    "\n",
    "ax[1, 3].set_xlabel('Alpha', fontsize=fontsize)\n",
    "ax[1, 3].set_yscale('log')\n",
    "ax[1, 3].tick_params(axis='both', labelsize=fontsize - 6)\n",
    "# ax[1, 3].xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=0))\n",
    "ax[1, 3].grid(ls='--')\n",
    "ax[1, 3].errorbar(\n",
    "    alphas, aps_4_count.mean(1), yerr=aps_4_count.std(1), ls='-', lw=lw_aps, elinewidth=elinewidth,capsize=capsize,alpha=alpha,\n",
    "    label='Ordinal APS',\n",
    ")\n",
    "ax[1, 3].errorbar(\n",
    "    alphas, lac_4_count.mean(1), yerr=lac_4_count.std(1), ls=':', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    "    label='Naive LAC', \n",
    ")\n",
    "ax[1, 3].errorbar(\n",
    "    alphas, cdf_4_count.mean(1), yerr=cdf_4_count.std(1), ls='--', lw=lw,elinewidth=elinewidth,capsize=capsize,\n",
    "    label='Ordinal CDF',\n",
    ")\n",
    "\n",
    "fig.legend(fontsize=fontsize + 8, bbox_to_anchor=(0.88, 0.02), ncol=3)\n",
    "fig.tight_layout()\n",
    "plt.savefig('../figures/line-comparison-by-size.eps', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_index = 1\n",
    "print('CLASS CONDITIONAL'.center(40, '-'))\n",
    "print('COVERAGE')\n",
    "print('LAC')\n",
    "print(f'{lac_neg_coverage[alpha_index].mean():.0%} +/- {lac_neg_coverage[alpha_index].std():.0%}')\n",
    "print(f'{lac_mil_coverage[alpha_index].mean():.0%} +/- {lac_mil_coverage[alpha_index].std():.0%}')\n",
    "print(f'{lac_mod_coverage[alpha_index].mean():.0%} +/- {lac_mod_coverage[alpha_index].std():.0%}')\n",
    "print(f'{lac_sev_coverage[alpha_index].mean():.0%} +/- {lac_sev_coverage[alpha_index].std():.0%}')\n",
    "print('CDF')\n",
    "print(f'{cdf_neg_coverage[alpha_index].mean():.0%} +/- {cdf_neg_coverage[alpha_index].std():.0%}')\n",
    "print(f'{cdf_mil_coverage[alpha_index].mean():.0%} +/- {cdf_mil_coverage[alpha_index].std():.0%}')\n",
    "print(f'{cdf_mod_coverage[alpha_index].mean():.0%} +/- {cdf_mod_coverage[alpha_index].std():.0%}')\n",
    "print(f'{cdf_sev_coverage[alpha_index].mean():.0%} +/- {cdf_sev_coverage[alpha_index].std():.0%}')\n",
    "print('APS')\n",
    "print(f'{aps_neg_coverage[alpha_index].mean():.0%} +/- {aps_neg_coverage[alpha_index].std():.0%}')\n",
    "print(f'{aps_mil_coverage[alpha_index].mean():.0%} +/- {aps_mil_coverage[alpha_index].std():.0%}')\n",
    "print(f'{aps_mod_coverage[alpha_index].mean():.0%} +/- {aps_mod_coverage[alpha_index].std():.0%}')\n",
    "print(f'{aps_sev_coverage[alpha_index].mean():.0%} +/- {aps_sev_coverage[alpha_index].std():.0%}')\n",
    "\n",
    "print('\\nSET SIZE')\n",
    "print('LAC')\n",
    "print(f'{lac_neg_size[alpha_index].mean():.1f} +/- {lac_neg_size[alpha_index].std():.1f}')\n",
    "print(f'{lac_mil_size[alpha_index].mean():.1f} +/- {lac_mil_size[alpha_index].std():.1f}')\n",
    "print(f'{lac_mod_size[alpha_index].mean():.1f} +/- {lac_mod_size[alpha_index].std():.1f}')\n",
    "print(f'{lac_sev_size[alpha_index].mean():.1f} +/- {lac_sev_size[alpha_index].std():.1f}')\n",
    "print('CDF')\n",
    "print(f'{cdf_neg_size[alpha_index].mean():.1f} +/- {cdf_neg_size[alpha_index].std():.1f}')\n",
    "print(f'{cdf_mil_size[alpha_index].mean():.1f} +/- {cdf_mil_size[alpha_index].std():.1f}')\n",
    "print(f'{cdf_mod_size[alpha_index].mean():.1f} +/- {cdf_mod_size[alpha_index].std():.1f}')\n",
    "print(f'{cdf_sev_size[alpha_index].mean():.1f} +/- {cdf_sev_size[alpha_index].std():.1f}')\n",
    "print('APS')\n",
    "print(f'{aps_neg_size[alpha_index].mean():.1f} +/- {aps_neg_size[alpha_index].std():.1f}')\n",
    "print(f'{aps_mil_size[alpha_index].mean():.1f} +/- {aps_mil_size[alpha_index].std():.1f}')\n",
    "print(f'{aps_mod_size[alpha_index].mean():.1f} +/- {aps_mod_size[alpha_index].std():.1f}')\n",
    "print(f'{aps_sev_size[alpha_index].mean():.1f} +/- {aps_sev_size[alpha_index].std():.1f}')\n",
    "\n",
    "print('SIZE CONDITIONAL'.center(40, '-'))\n",
    "print('COVERAGE')\n",
    "print('LAC')\n",
    "print(f'{lac_1_coverage[alpha_index].mean():.0%} +/- {lac_1_coverage[alpha_index].std():.0%}')\n",
    "print(f'{lac_2_coverage[alpha_index].mean():.0%} +/- {lac_2_coverage[alpha_index].std():.0%}')\n",
    "print(f'{lac_3_coverage[alpha_index].mean():.0%} +/- {lac_3_coverage[alpha_index].std():.0%}')\n",
    "print(f'{lac_4_coverage[alpha_index].mean():.0%} +/- {lac_4_coverage[alpha_index].std():.0%}')\n",
    "print('CDF')\n",
    "print(f'{cdf_1_coverage[alpha_index].mean():.0%} +/- {cdf_1_coverage[alpha_index].std():.0%}')\n",
    "print(f'{cdf_2_coverage[alpha_index].mean():.0%} +/- {cdf_2_coverage[alpha_index].std():.0%}')\n",
    "print(f'{cdf_3_coverage[alpha_index].mean():.0%} +/- {cdf_3_coverage[alpha_index].std():.0%}')\n",
    "print(f'{cdf_4_coverage[alpha_index].mean():.0%} +/- {cdf_4_coverage[alpha_index].std():.0%}')\n",
    "print('APS')\n",
    "print(f'{aps_1_coverage[alpha_index].mean():.0%} +/- {aps_1_coverage[alpha_index].std():.0%}')\n",
    "print(f'{aps_2_coverage[alpha_index].mean():.0%} +/- {aps_2_coverage[alpha_index].std():.0%}')\n",
    "print(f'{aps_3_coverage[alpha_index].mean():.0%} +/- {aps_3_coverage[alpha_index].std():.0%}')\n",
    "print(f'{aps_4_coverage[alpha_index].mean():.0%} +/- {aps_4_coverage[alpha_index].std():.0%}')\n",
    "\n",
    "print('\\nCOUNT')\n",
    "print('LAC')\n",
    "print(f'{lac_1_count[alpha_index].mean():.0f} +/- {lac_1_count[alpha_index].std():.0f}')\n",
    "print(f'{lac_2_count[alpha_index].mean():.0f} +/- {lac_2_count[alpha_index].std():.0f}')\n",
    "print(f'{lac_3_count[alpha_index].mean():.0f} +/- {lac_3_count[alpha_index].std():.0f}')\n",
    "print(f'{lac_4_count[alpha_index].mean():.0f} +/- {lac_4_count[alpha_index].std():.0f}')\n",
    "print('CDF')\n",
    "print(f'{cdf_1_count[alpha_index].mean():.0f} +/- {cdf_1_count[alpha_index].std():.0f}')\n",
    "print(f'{cdf_2_count[alpha_index].mean():.0f} +/- {cdf_2_count[alpha_index].std():.0f}')\n",
    "print(f'{cdf_3_count[alpha_index].mean():.0f} +/- {cdf_3_count[alpha_index].std():.0f}')\n",
    "print(f'{cdf_4_count[alpha_index].mean():.0f} +/- {cdf_4_count[alpha_index].std():.0f}')\n",
    "print('APS')\n",
    "print(f'{aps_1_count[alpha_index].mean():.0f} +/- {aps_1_count[alpha_index].std():.0f}')\n",
    "print(f'{aps_2_count[alpha_index].mean():.0f} +/- {aps_2_count[alpha_index].std():.0f}')\n",
    "print(f'{aps_3_count[alpha_index].mean():.0f} +/- {aps_3_count[alpha_index].std():.0f}')\n",
    "print(f'{aps_4_count[alpha_index].mean():.0f} +/- {aps_4_count[alpha_index].std():.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 2: Patient level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_percent = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "rand_patients = np.random.permutation(list(clean_patient_map.keys()))\n",
    "n = len(rand_patients)\n",
    "split = int(n * cal_percent)\n",
    "cal_patients = rand_patients[:split]\n",
    "val_patients = rand_patients[split:]\n",
    "cal_indexes = sum([clean_patient_map[k] for k in cal_patients], [])\n",
    "val_indexes = sum([clean_patient_map[k] for k in val_patients], [])\n",
    "cal_scores = clean_score_array[cal_indexes]\n",
    "cal_labels = clean_label_array[cal_indexes]\n",
    "val_scores = clean_score_array[val_indexes]\n",
    "val_labels = clean_label_array[val_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lac_qhat = np.quantile(\n",
    "    conformal_demo.lac_score_function(\n",
    "        np.copy(cal_scores),\n",
    "        np.copy(cal_labels)\n",
    "    ),\n",
    "    np.ceil((n + 1)*(1 - alpha))/n,\n",
    "    interpolation='higher',\n",
    ")\n",
    "\n",
    "cdf_qhat = np.quantile(\n",
    "    conformal_demo.cdf_naive_ordinal_score_function(\n",
    "        np.copy(cal_scores),\n",
    "        np.copy(cal_labels)\n",
    "    ),\n",
    "    np.ceil((n + 1)*(1 - alpha))/n,\n",
    "    interpolation='higher',\n",
    ")\n",
    "\n",
    "aps_qhat = conformal_demo.get_qhat_ordinal_aps(\n",
    "    conformal_demo.ordinal_aps_prediction,\n",
    "    np.copy(cal_scores),\n",
    "    np.copy(cal_labels),\n",
    "    alpha,\n",
    ")\n",
    "\n",
    "lac_pred = conformal_demo.lac_prediction(clean_score_array, lac_qhat)\n",
    "cdf_pred = conformal_demo.cdf_naive_ordinal_prediction(clean_score_array, cdf_qhat)\n",
    "aps_pred = conformal_demo.ordinal_aps_prediction(clean_score_array, aps_qhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_clean_patient_map = {\n",
    "    index: k for k, v in clean_patient_map.items() for index in v\n",
    "}\n",
    "val_patient_map = {idx: reverse_clean_patient_map[idx] for idx in val_indexes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lac_patient_scores = collections.defaultdict(list)\n",
    "lac_patient_sizes = collections.defaultdict(list)\n",
    "cdf_patient_scores = collections.defaultdict(list)\n",
    "cdf_patient_sizes = collections.defaultdict(list)\n",
    "aps_patient_scores = collections.defaultdict(list)\n",
    "aps_patient_sizes = collections.defaultdict(list)\n",
    "\n",
    "for index, patient in val_patient_map.items():\n",
    "    lac_pred_set = lac_pred[index]\n",
    "    cdf_pred_set = cdf_pred[index]\n",
    "    aps_pred_set = aps_pred[index]\n",
    "    lac_patient_scores[patient].append(np.arange(4)[lac_pred_set].mean())\n",
    "    cdf_patient_scores[patient].append(np.arange(4)[cdf_pred_set].mean())\n",
    "    aps_patient_scores[patient].append(np.arange(4)[aps_pred_set].mean())\n",
    "    lac_patient_sizes[patient].append(lac_pred_set.sum())\n",
    "    cdf_patient_sizes[patient].append(cdf_pred_set.sum())\n",
    "    aps_patient_sizes[patient].append(aps_pred_set.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 28\n",
    "_alpha = 0.5\n",
    "xticks = np.arange(0, 3.1, 1)\n",
    "yticks = np.arange(1, 4.1, 1)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(16, 4), sharey=False)\n",
    "\n",
    "ax[0].set_xticks(xticks)\n",
    "ax[0].set_xticklabels(xticks, fontsize=fontsize - 4)\n",
    "ax[0].xaxis.set_major_formatter(mtick.FormatStrFormatter('%.0f'))\n",
    "ax[0].set_yticks(yticks)\n",
    "ax[0].set_yticklabels(yticks, fontsize=fontsize - 4)\n",
    "ax[0].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.0f'))\n",
    "ax[0].set_title('Ordinal APS', fontsize=fontsize)\n",
    "ax[0].set_xlabel('Predicted severity', fontsize=fontsize)\n",
    "ax[0].set_ylabel('Set size', fontsize=fontsize)\n",
    "ax[0].set_xlim(0, 3)\n",
    "ax[0].set_ylim(1, 4)\n",
    "ax[0].grid(ls='--')\n",
    "ax[0].scatter(\n",
    "    [np.mean(v) for v in aps_patient_scores.values()],\n",
    "    [np.mean(v) for v in aps_patient_sizes.values()],\n",
    "    alpha=_alpha,\n",
    "    c='C0',\n",
    ")\n",
    "ax[1].set_xticks(xticks)\n",
    "ax[1].set_xticklabels(xticks, fontsize=fontsize - 4)\n",
    "ax[1].xaxis.set_major_formatter(mtick.FormatStrFormatter('%.0f'))\n",
    "ax[1].set_yticks(yticks)\n",
    "ax[1].set_yticklabels(yticks, fontsize=fontsize - 4)\n",
    "ax[1].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.0f'))\n",
    "ax[1].set_title('Naive LAC', fontsize=fontsize)\n",
    "ax[1].set_xlabel('Predicted severity', fontsize=fontsize)\n",
    "ax[1].set_ylabel('Set size', fontsize=fontsize)\n",
    "ax[1].set_xlim(0, 3)\n",
    "ax[1].set_ylim(1, 4)\n",
    "ax[1].grid(ls='--')\n",
    "ax[1].scatter(\n",
    "    [np.mean(v) for v in lac_patient_scores.values()],\n",
    "    [np.mean(v) for v in lac_patient_sizes.values()],\n",
    "    alpha=_alpha,\n",
    "    c='C1',\n",
    ")\n",
    "\n",
    "ax[2].set_xticks(xticks)\n",
    "ax[2].set_xticklabels(xticks, fontsize=fontsize - 4)\n",
    "ax[2].xaxis.set_major_formatter(mtick.FormatStrFormatter('%.0f'))\n",
    "ax[2].set_yticks(yticks)\n",
    "ax[2].set_yticklabels(yticks, fontsize=fontsize - 4)\n",
    "ax[2].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.0f'))\n",
    "ax[2].set_title('Ordinal CDF', fontsize=fontsize)\n",
    "ax[2].set_xlabel('Predicted severity', fontsize=fontsize)\n",
    "ax[2].set_ylabel('Set size', fontsize=fontsize)\n",
    "ax[2].set_xlim(0, 3)\n",
    "ax[2].set_ylim(1, 4)\n",
    "ax[2].grid(ls='--')\n",
    "ax[2].scatter(\n",
    "    [np.mean(v) for v in cdf_patient_scores.values()],\n",
    "    [np.mean(v) for v in cdf_patient_sizes.values()],\n",
    "    alpha=_alpha,\n",
    "    c='C2',\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\n",
    "    '../figures/patient-severity-vs-size.eps', bbox_inches='tight'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_severity_scores(pred, labels, cls_idx=3):\n",
    "    get_mask = lambda x: np.tile(np.arange(4), [x.shape[0], 1])\n",
    "    class_pred = pred[labels == cls_idx]\n",
    "    masked_class_pred = class_pred * get_mask(class_pred)\n",
    "    nonzero_count = np.count_nonzero(masked_class_pred, -1)\n",
    "\n",
    "    # avoid dividing by 0\n",
    "    safe_nonzero_count = np.maximum(nonzero_count, 1) \n",
    "   \n",
    "    return masked_class_pred.sum(-1) / safe_nonzero_count\n",
    "\n",
    "aps_scores_sev = get_severity_scores(aps_pred[val_indexes], val_labels, cls_idx=3)\n",
    "aps_scores_mod = get_severity_scores(aps_pred[val_indexes], val_labels, cls_idx=2)\n",
    "aps_scores_mil = get_severity_scores(aps_pred[val_indexes], val_labels, cls_idx=1)\n",
    "aps_scores_neg = get_severity_scores(aps_pred[val_indexes], val_labels, cls_idx=0)\n",
    "\n",
    "cdf_scores_sev = get_severity_scores(cdf_pred[val_indexes], val_labels, cls_idx=3)\n",
    "cdf_scores_mod = get_severity_scores(cdf_pred[val_indexes], val_labels, cls_idx=2)\n",
    "cdf_scores_mil = get_severity_scores(cdf_pred[val_indexes], val_labels, cls_idx=1)\n",
    "cdf_scores_neg = get_severity_scores(cdf_pred[val_indexes], val_labels, cls_idx=0)\n",
    "\n",
    "lac_scores_sev = get_severity_scores(lac_pred[val_indexes], val_labels, cls_idx=3)\n",
    "lac_scores_mod = get_severity_scores(lac_pred[val_indexes], val_labels, cls_idx=2)\n",
    "lac_scores_mil = get_severity_scores(lac_pred[val_indexes], val_labels, cls_idx=1)\n",
    "lac_scores_neg = get_severity_scores(lac_pred[val_indexes], val_labels, cls_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting helper functions\n",
    "\n",
    "def set_box_color(bp, color):\n",
    "    # plt.setp(bp['boxes'], color=color)\n",
    "    plt.setp(bp['boxes'], facecolor=color)\n",
    "    plt.setp(bp['whiskers'], color=color)\n",
    "    # plt.setp(bp['caps'], color=color)\n",
    "    plt.setp(bp['medians'], color='k')\n",
    "    \n",
    "def set_violin_color(violin, color):\n",
    "    for pc in violin['bodies']:\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.5)\n",
    "        \n",
    "    violin['cmedians'].set_edgecolor(color)\n",
    "    violin['cmedians'].set_linewidth(6)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 32\n",
    "width = 0.25\n",
    "offset = 0.25\n",
    "showmedians=True\n",
    "showextrema=False\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.xticks(list(range(4)), labels=['Normal', 'Mild', 'Moderate', 'Severe'], fontsize=fontsize)\n",
    "plt.xlabel('True label', fontsize=fontsize)\n",
    "plt.ylabel('Predicted\\nSeverity', fontsize=fontsize + 4)\n",
    "plt.xticks(list(range(0, 4)), fontsize=fontsize)\n",
    "plt.yticks(np.arange(0, 5, 1.0), fontsize=fontsize - 8)\n",
    "\n",
    "violin = plt.violinplot(\n",
    "    aps_scores_neg,\n",
    "    positions=[0 - offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C0')\n",
    "violin = plt.violinplot(\n",
    "    aps_scores_mil,\n",
    "    positions=[1 - offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C0')\n",
    "violin = plt.violinplot(\n",
    "    aps_scores_mod,\n",
    "    positions=[2 - offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C0')\n",
    "violin = plt.violinplot(\n",
    "    aps_scores_sev,\n",
    "    positions=[3 - offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C0')\n",
    "\n",
    "violin = plt.violinplot(\n",
    "    lac_scores_neg,\n",
    "    positions=[0],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C1')\n",
    "violin = plt.violinplot(\n",
    "    lac_scores_mil,\n",
    "    positions=[1],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C1')\n",
    "violin = plt.violinplot(\n",
    "    lac_scores_mod,\n",
    "    positions=[2],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C1')\n",
    "violin = plt.violinplot(\n",
    "    lac_scores_sev,\n",
    "    positions=[3],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C1')\n",
    "\n",
    "violin = plt.violinplot(\n",
    "    cdf_scores_neg,\n",
    "    positions=[0 + offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C2')\n",
    "violin = plt.violinplot(\n",
    "    cdf_scores_mil,\n",
    "    positions=[1 + offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C2')\n",
    "violin = plt.violinplot(\n",
    "    cdf_scores_mod,\n",
    "    positions=[2 + offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C2')\n",
    "violin = plt.violinplot(\n",
    "    cdf_scores_sev,\n",
    "    positions=[3 + offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C2')\n",
    "\n",
    "\n",
    "a, = plt.plot([0,0], [0, 0],'C0', lw=12, alpha=0.7)\n",
    "b, = plt.plot([0,0], [0, 0],'C1', lw=12, alpha=0.7)\n",
    "c, = plt.plot([0,0], [0,0],'C2', lw=12, alpha=0.7)\n",
    "\n",
    "plt.grid(axis='y', ls='--')\n",
    "plt.legend(\n",
    "    (a, b, c),\n",
    "    ['Ordinal APS', 'Naive LAC', 'Ordinal CDF'],\n",
    "    fontsize=fontsize, \n",
    "    bbox_to_anchor=(0.5, 1.3), \n",
    "    ncol=3,\n",
    "    # mode='expand',\n",
    "    loc='upper center'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/compare-severity-scores.eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_severity_sizes(pred, labels, cls_idx=3):\n",
    "    class_pred = pred[labels == cls_idx]\n",
    "    nonzero_count = np.count_nonzero(class_pred, -1)\n",
    "    return nonzero_count\n",
    "\n",
    "aps_sizes_sev = get_severity_sizes(aps_pred[val_indexes], val_labels, cls_idx=3)\n",
    "aps_sizes_mod = get_severity_sizes(aps_pred[val_indexes], val_labels, cls_idx=2)\n",
    "aps_sizes_mil = get_severity_sizes(aps_pred[val_indexes], val_labels, cls_idx=1)\n",
    "aps_sizes_neg = get_severity_sizes(aps_pred[val_indexes], val_labels, cls_idx=0)\n",
    "\n",
    "cdf_sizes_sev = get_severity_sizes(cdf_pred[val_indexes], val_labels, cls_idx=3)\n",
    "cdf_sizes_mod = get_severity_sizes(cdf_pred[val_indexes], val_labels, cls_idx=2)\n",
    "cdf_sizes_mil = get_severity_sizes(cdf_pred[val_indexes], val_labels, cls_idx=1)\n",
    "cdf_sizes_neg = get_severity_sizes(cdf_pred[val_indexes], val_labels, cls_idx=0)\n",
    "\n",
    "lac_sizes_sev = get_severity_sizes(lac_pred[val_indexes], val_labels, cls_idx=3)\n",
    "lac_sizes_mod = get_severity_sizes(lac_pred[val_indexes], val_labels, cls_idx=2)\n",
    "lac_sizes_mil = get_severity_sizes(lac_pred[val_indexes], val_labels, cls_idx=1)\n",
    "lac_sizes_neg = get_severity_sizes(lac_pred[val_indexes], val_labels, cls_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 24\n",
    "width = 0.12\n",
    "\n",
    "def set_box_color(bp, color):\n",
    "    # plt.setp(bp['boxes'], color=color)\n",
    "    plt.setp(bp['whiskers'], color=color)\n",
    "    plt.setp(bp['caps'], color=color)\n",
    "    plt.setp(bp['medians'], color='k')\n",
    "    \n",
    "fontsize = 32\n",
    "width = 0.25\n",
    "offset = 0.25\n",
    "showmedians=True\n",
    "showextrema=False\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.xticks(list(range(4)), labels=['Normal', 'Mild', 'Moderate', 'Severe'], fontsize=fontsize)\n",
    "plt.xlabel('True label', fontsize=fontsize)\n",
    "plt.ylabel('Set size', fontsize=fontsize + 4)\n",
    "plt.xticks(list(range(0, 4)), fontsize=fontsize)\n",
    "plt.yticks(np.arange(0, 5, 1.0), fontsize=fontsize - 8)\n",
    "# plt.xlim(-1, 5)\n",
    "plt.ylim(1, 4.1)\n",
    "violin = plt.violinplot(\n",
    "    aps_sizes_neg,\n",
    "    positions=[0 - offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C0')\n",
    "violin = plt.violinplot(\n",
    "    aps_sizes_mil,\n",
    "    positions=[1 - offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C0')\n",
    "violin = plt.violinplot(\n",
    "    aps_sizes_mod,\n",
    "    positions=[2 - offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C0')\n",
    "violin = plt.violinplot(\n",
    "    aps_sizes_sev,\n",
    "    positions=[3 - offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C0')\n",
    "\n",
    "violin = plt.violinplot(\n",
    "    lac_sizes_neg,\n",
    "    positions=[0],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C1')\n",
    "violin = plt.violinplot(\n",
    "    lac_sizes_mil,\n",
    "    positions=[1],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C1')\n",
    "violin = plt.violinplot(\n",
    "    lac_sizes_mod,\n",
    "    positions=[2],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C1')\n",
    "violin = plt.violinplot(\n",
    "    lac_sizes_sev,\n",
    "    positions=[3],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C1')\n",
    "\n",
    "violin = plt.violinplot(\n",
    "    cdf_sizes_neg,\n",
    "    positions=[0 + offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C2')\n",
    "violin = plt.violinplot(\n",
    "    cdf_sizes_mil,\n",
    "    positions=[1 + offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C2')\n",
    "violin = plt.violinplot(\n",
    "    cdf_sizes_mod,\n",
    "    positions=[2 + offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C2')\n",
    "violin = plt.violinplot(\n",
    "    cdf_sizes_sev,\n",
    "    positions=[3 + offset],\n",
    "    widths=width,\n",
    "    showmedians=showmedians,\n",
    "    showextrema=showextrema,\n",
    ")\n",
    "set_violin_color(violin, 'C2')\n",
    "\n",
    "\n",
    "a, = plt.plot([0,0], [0, 0],'C0', lw=12, alpha=0.7)\n",
    "b, = plt.plot([0,0], [0, 0],'C1', lw=12, alpha=0.7)\n",
    "c, = plt.plot([0,0], [0,0],'C2', lw=12, alpha=0.7)\n",
    "\n",
    "plt.grid(axis='y', ls='--')\n",
    "plt.legend(\n",
    "    (a, b, c),\n",
    "    ['Ordinal APS', 'LAC', 'CDF'],\n",
    "    fontsize=fontsize, \n",
    "    bbox_to_anchor=(0.5, 1.3), \n",
    "    ncol=3,\n",
    "    # mode='expand',\n",
    "    loc='upper center'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/compare-severity-sizes.eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 3: Clinical use-case of conformal uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_lac_severity = {k: sum(v) for k, v in lac_patient_scores.items()}\n",
    "agg_cdf_severity = {k: sum(v) for k, v in cdf_patient_scores.items()}\n",
    "agg_aps_severity = {k: sum(v) for k, v in aps_patient_scores.items()}\n",
    "\n",
    "agg_lac_uncertainty = {k: sum(v) for k, v in lac_patient_sizes.items()}\n",
    "agg_cdf_uncertainty = {k: sum(v) for k, v in cdf_patient_sizes.items()}\n",
    "agg_aps_uncertainty = {k: sum(v) for k, v in aps_patient_sizes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_severity_patients = {\n",
    "    x[0]: x[1] for x in sorted(agg_aps_severity.items(), key=lambda x: x[1], reverse=True)\n",
    "    if x[1] < 10\n",
    "}\n",
    "med_severity_patients = {\n",
    "    x[0]: x[1] for x in sorted(agg_aps_severity.items(), key=lambda x: x[1], reverse=True)\n",
    "    if 10 <= x[1] < 20\n",
    "}\n",
    "high_severity_patients = {\n",
    "    x[0]: x[1] for x in sorted(agg_aps_severity.items(), key=lambda x: x[1], reverse=True)\n",
    "    if 20 <= x[1]\n",
    "}\n",
    "\n",
    "most_uncertain_cases_low_severity = sorted([(k, v, agg_aps_uncertainty[k]) for k, v in low_severity_patients.items()], key=lambda x: x[2], reverse=True)[:4]\n",
    "most_uncertain_cases_med_severity = sorted([(k, v, agg_aps_uncertainty[k]) for k, v in med_severity_patients.items()], key=lambda x: x[2], reverse=True)[:4]\n",
    "most_uncertain_cases_high_severity = sorted([(k, v, agg_aps_uncertainty[k]) for k, v in high_severity_patients.items()], key=lambda x: x[2], reverse=True)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cases_uncertainty = [\n",
    "    x[2] for x in \n",
    "    most_uncertain_cases_low_severity + most_uncertain_cases_med_severity + most_uncertain_cases_high_severity \n",
    "]\n",
    "selected_cases_severity = [\n",
    "    x[1] for x in \n",
    "    most_uncertain_cases_low_severity + most_uncertain_cases_med_severity + most_uncertain_cases_high_severity \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 28\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.grid(ls='--')\n",
    "plt.ylabel('Predicted\\nseverity', fontsize=fontsize)\n",
    "plt.xlabel('Uncertainty score', fontsize=fontsize)\n",
    "plt.yticks(list(range(0, 50, 10)), fontsize=fontsize-8)\n",
    "plt.xticks(fontsize=fontsize-8)\n",
    "# plt.xlim(-1, 45)\n",
    "plt.ylim(-1, 36)\n",
    "y = list(agg_aps_severity.values())\n",
    "x = list(agg_aps_uncertainty.values())\n",
    "plt.scatter(\n",
    "    x, y, alpha=0.5, c='C7'\n",
    ")\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), ls='--', lw=3, c='C0')\n",
    "plt.scatter(\n",
    "    [x[2] for x in most_uncertain_cases_low_severity],\n",
    "    [x[1] for x in most_uncertain_cases_low_severity],\n",
    "    # (33, 32, 32, 32),\n",
    "    # (9, 9, 8, 8),\n",
    "    marker='*',\n",
    "    s=300,\n",
    "    color='C2'\n",
    ")\n",
    "plt.scatter(\n",
    "    [x[2] for x in most_uncertain_cases_med_severity],\n",
    "    [x[1] for x in most_uncertain_cases_med_severity],\n",
    "    # (39, 33, 35),\n",
    "    # (16.5, 11.5, 18.5),\n",
    "    marker='*',\n",
    "    s=300,\n",
    "    color='C1'\n",
    ")\n",
    "plt.scatter(\n",
    "    [x[2] for x in most_uncertain_cases_high_severity],\n",
    "    [x[1] for x in most_uncertain_cases_high_severity],\n",
    "    # (30, 31, 36, 40),\n",
    "    # (21, 22.5, 26, 29),\n",
    "    marker='*',\n",
    "    s=300,\n",
    "    color='C3'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/patient-severity-uncertainty-correlation.eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pearson', scipy.stats.pearsonr(x, y))\n",
    "print('spearman', scipy.stats.spearmanr(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     *label_df.iloc[\n",
    "#         [x[0] for x in most_uncertain_cases_low_severity] +\n",
    "#         [x[0] for x in most_uncertain_cases_med_severity] +\n",
    "#         [x[0] for x in most_uncertain_cases_high_severity]\n",
    "#     ].index, sep='\\n'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
